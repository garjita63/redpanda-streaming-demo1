{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fc65e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd846963",
   "metadata": {},
   "source": [
    "- cd /mnt/d/zde/data-engineering-zoomcamp/cohorts/2024/06-streaming\n",
    "- docker-compose up -d\n",
    "- docker ps\n",
    "- dockwer exec -it redpanda-1 bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a9039",
   "metadata": {},
   "source": [
    "# Question 1: Redpanda version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c57ef7",
   "metadata": {},
   "source": [
    "- rpk help \n",
    "- rpk version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc70da",
   "metadata": {},
   "source": [
    "# Question 2. Creating a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ff481",
   "metadata": {},
   "source": [
    "- create topic test-topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b03ed",
   "metadata": {},
   "source": [
    "# Question 3. Connecting to the Kafka server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea7bf5",
   "metadata": {},
   "source": [
    "- pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844cbf07",
   "metadata": {},
   "source": [
    "**Answer 3 : True**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11292f9",
   "metadata": {},
   "source": [
    "# Question 4. Sending data to the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "t01 = time.time()\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Sending message took {(t01 - t0):.2f} seconds')\n",
    "print(f'Flus took {(t1 - t01):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a09e9",
   "metadata": {},
   "source": [
    "How much time did it take? Where did it spend most of the time?\n",
    "\n",
    "- Sending the messages\n",
    "- Flushing\n",
    "- Both took approximately the same amount of time\n",
    "\n",
    "**Answer 4 : Sending the messages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fb6f3",
   "metadata": {},
   "source": [
    "# Question 5: Sending the Trip Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1fbc6",
   "metadata": {},
   "source": [
    "## Sending the taxi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa9141",
   "metadata": {},
   "source": [
    "Now let's send our actual data:\n",
    "\n",
    "Read the green csv.gz file\n",
    "\n",
    "We will only need these columns:\n",
    "- 'lpep_pickup_datetime',\n",
    "- 'lpep_dropoff_datetime',\n",
    "- 'PULocationID',\n",
    "- 'DOLocationID',\n",
    "- 'passenger_count',\n",
    "- 'trip_distance',\n",
    "- 'tip_amount'\n",
    "\n",
    "Iterate over the records in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a804342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and define dataFrame\n",
    "import pandas as pd\n",
    "df_green =  pd.read_csv(\"green_tripdata_2019-10.csv.gz\")\n",
    "\n",
    "columns_to_list = ['lpep_pickup_datetime',\n",
    "'lpep_dropoff_datetime',\n",
    "'PULocationID',\n",
    "'DOLocationID',\n",
    "'passenger_count',\n",
    "'trip_distance',\n",
    "'tip_amount']\n",
    "\n",
    "\n",
    "df_green = df_green[columns_to_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "\n",
    "t01 = time.time()\n",
    "print('\\n')\n",
    "print(f'Sending data took {(t01 - t0):.2f} seconds')\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Flusing took {(t1 - t01):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47796bfe",
   "metadata": {},
   "source": [
    "- Create a topic green-trips and send the data there\n",
    "- How much time in seconds did it take? (You can round it to a whole number)\n",
    "- Make sure you don't include sleeps in your code\n",
    "\n",
    "**Answer 5 : 7.32 seconds**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc80990",
   "metadata": {},
   "source": [
    "# Question 6. Parsing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7232b7f",
   "metadata": {},
   "source": [
    "## Creating the PySpark consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0448d5",
   "metadata": {},
   "source": [
    "Now let's read the data with PySpark.\n",
    "\n",
    "Spark needs a library (jar) to be able to connect to Kafka, so we need to tell PySpark that it needs to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db58695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f2bcd",
   "metadata": {},
   "source": [
    "Now we can connect to the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e60a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1517248",
   "metadata": {},
   "source": [
    "In order to test that we can consume from the stream, let's see what will be the first record there.\n",
    "\n",
    "In Spark streaming, the stream is represented as a sequence of small batches, each batch being a small RDD (or a small dataframe).\n",
    "\n",
    "So we can execute a function over each mini-batch. Let's run take(1) there to see what do we have in the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87404fe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8b121",
   "metadata": {},
   "source": [
    "You should see a record like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584b62d",
   "metadata": {},
   "source": [
    "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 4, 7, 20, 34, 5, 862000), timestampType=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946a7f8",
   "metadata": {},
   "source": [
    "Now let's stop the query, so it doesn't keep consuming messages from the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e67fe9",
   "metadata": {},
   "source": [
    "The data is JSON, but currently it's in binary format. We need to parse it and turn it into a streaming dataframe with proper columns.\n",
    "\n",
    "Similarly to PySpark, we define the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a788691",
   "metadata": {},
   "source": [
    "And apply this schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe65ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb6159",
   "metadata": {},
   "source": [
    "How does the record look after parsing? Copy the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d421b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8eccd",
   "metadata": {},
   "source": [
    "**Answer 6** :\n",
    "\n",
    "**DataFrame[lpep_pickup_datetime: string, lpep_dropoff_datetime: string, PULocationID: int, DOLocationID: int, passenger_count: double, trip_distance: double, tip_amount: double]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4d6e3",
   "metadata": {},
   "source": [
    "# Question 7: Most popular destination\n",
    "\n",
    "Now let's finally do some streaming analytics. We will see what's the most popular destination currently based on our stream of data (which ideally we should have sent with delays like we did in workshop 2)\n",
    "\n",
    "This is how you can do it:\n",
    "\n",
    "- Add a column \"timestamp\" using the current_timestamp function\n",
    "- Group by:\n",
    "    - 5 minutes window based on the timestamp column (F.window(col(\"timestamp\"), \"5 minutes\"))\n",
    "    - \"DOLocationID\"\n",
    "- Order by count\n",
    "\n",
    "You can print the output to the console using this code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb595837",
   "metadata": {},
   "source": [
    "# do some streaming analytics.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_green_stream = green_stream.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "popular_destinations = df_green_stream \\\n",
    "    .groupBy(F.window(col(\"timestamp\"), \"5 minutes\"), df_green_stream.DOLocationID) \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc()) \\\n",
    "    .limit(1) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .start()\n",
    "\n",
    "popular_destinations.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf926295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see most_popular_destination.ipynb script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37cdaa",
   "metadata": {},
   "source": [
    "**Answer 7 :**\n",
    "\n",
    "- LocationID : 74\n",
    "- Zone : East Harlem North\n",
    "- number_dropoff : 17741"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caaf73e",
   "metadata": {},
   "source": [
    "# Calling another ipynb script\n",
    "\n",
    "Example : **call most_popular_destination.ipynb** scipt\n",
    "\n",
    "Refference : https://stackoverflow.com/questions/20186344/importing-an-ipynb-file-from-another-ipynb-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ca0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import most_popular_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da46b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
